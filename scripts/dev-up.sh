#!/bin/bash
# scripts/dev-up.sh
# Local Talos cluster creation script for InferaDB development
# Creates a Talos cluster using Docker provisioner with default flannel CNI
# and deploys the full InferaDB stack (FDB, engine, control, dashboard)
#
# Features:
# - Tailscale integration for automatic HTTPS
# - Services accessible at https://inferadb-api.<tailnet>.ts.net
# - Mirrors production architecture for dev/prod parity

set -euo pipefail

CLUSTER_NAME="inferadb-dev"
KUBE_CONTEXT="admin@${CLUSTER_NAME}"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
DEPLOY_DIR="${SCRIPT_DIR}/.."

# Parse arguments
BUILD_IMAGES=true
while [[ $# -gt 0 ]]; do
  case $1 in
    --skip-build)
      BUILD_IMAGES=false
      shift
      ;;
    --help|-h)
      echo "Usage: $0 [OPTIONS]"
      echo ""
      echo "Creates a local Talos Kubernetes cluster and deploys the full InferaDB stack."
      echo "Uses Tailscale for automatic HTTPS access."
      echo ""
      echo "First-time setup:"
      echo "  1. Enable HTTPS at https://login.tailscale.com/admin/dns"
      echo "     - Scroll to 'HTTPS Certificates' and click 'Enable HTTPS'"
      echo "  2. Create tags at https://login.tailscale.com/admin/acls/tags"
      echo "     - Tag: k8s-operator, Owner: yourself"
      echo "     - Tag: k8s, Owner: tag:k8s-operator"
      echo "  3. Create OAuth client at https://login.tailscale.com/admin/settings/oauth"
      echo "     - Scopes: Devices→Core (R/W), Keys→Auth Keys (R/W)"
      echo "     - Tag both scopes with: k8s-operator"
      echo "  4. Run this script and enter credentials when prompted"
      echo "     (credentials are cached in ~/.config/inferadb/)"
      echo ""
      echo "Options:"
      echo "  --skip-build   Skip building container images (use existing images)"
      echo "  --help         Show this help message"
      echo ""
      echo "Examples:"
      echo "  $0               # Build images and deploy full stack"
      echo "  $0 --skip-build  # Deploy using existing images"
      echo ""
      echo "After setup, access:"
      echo "  https://inferadb-api.<tailnet>.ts.net       - API (control + engine)"
      echo "  https://inferadb-dashboard.<tailnet>.ts.net - Dashboard"
      exit 0
      ;;
    *)
      echo "Unknown option: $1"
      echo "Use --help for usage information"
      exit 1
      ;;
  esac
done

# =============================================================================
# Prerequisites Check
# =============================================================================
echo "=== Checking prerequisites ==="

# Tailscale credentials file location (for caching OAuth credentials)
TAILSCALE_CREDS_FILE="${HOME}/.config/inferadb/tailscale-credentials"

# Function to get tailnet info from local Tailscale CLI
get_tailnet_info() {
  if command -v tailscale &> /dev/null; then
    TAILSCALE_STATUS=$(tailscale status --json 2>/dev/null || echo "{}")
    if echo "${TAILSCALE_STATUS}" | grep -q '"BackendState"'; then
      # Extract DNS name (e.g., "evans-macbook-pro-m4.tail27bf77.ts.net.")
      DNS_NAME=$(echo "${TAILSCALE_STATUS}" | grep '"DNSName"' | head -1 | sed 's/.*"\([^"]*\.ts\.net\)\.".*/\1/')
      if [ -n "${DNS_NAME}" ]; then
        # Extract tailnet (everything after the first dot: "tail27bf77.ts.net")
        TAILNET_DOMAIN=$(echo "${DNS_NAME}" | cut -d. -f2-)
        echo "${TAILNET_DOMAIN}"
        return 0
      fi
    fi
  fi
  return 1
}

# Function to load cached credentials
load_cached_credentials() {
  if [ -f "${TAILSCALE_CREDS_FILE}" ]; then
    # Source the credentials file
    # shellcheck source=/dev/null
    source "${TAILSCALE_CREDS_FILE}"
    if [ -n "${TAILSCALE_CLIENT_ID:-}" ] && [ -n "${TAILSCALE_CLIENT_SECRET:-}" ]; then
      echo "Using cached Tailscale credentials from ${TAILSCALE_CREDS_FILE}"
      return 0
    fi
  fi
  return 1
}

# Function to save credentials for future use
save_credentials() {
  mkdir -p "$(dirname "${TAILSCALE_CREDS_FILE}")"
  cat > "${TAILSCALE_CREDS_FILE}" <<EOF
# Tailscale OAuth credentials for InferaDB development
# Generated by dev-up.sh
TAILSCALE_CLIENT_ID="${TAILSCALE_CLIENT_ID}"
TAILSCALE_CLIENT_SECRET="${TAILSCALE_CLIENT_SECRET}"
EOF
  chmod 600 "${TAILSCALE_CREDS_FILE}"
  echo "Credentials saved to ${TAILSCALE_CREDS_FILE} for future use"
}

# Try to load credentials from environment or cache
if [ -z "${TAILSCALE_CLIENT_ID:-}" ] || [ -z "${TAILSCALE_CLIENT_SECRET:-}" ]; then
  load_cached_credentials || true
fi

# Check if we have Tailscale credentials now
if [ -z "${TAILSCALE_CLIENT_ID:-}" ] || [ -z "${TAILSCALE_CLIENT_SECRET:-}" ]; then
  echo "Tailscale OAuth credentials required for Kubernetes operator."
  echo ""

  # Try to get tailnet info from local CLI for personalized instructions
  DETECTED_TAILNET=$(get_tailnet_info || echo "")

  if [ -n "${DETECTED_TAILNET}" ]; then
    echo "Detected tailnet: ${DETECTED_TAILNET}"
    echo ""
  fi

  echo "=== Setup Instructions ==="
  echo ""
  echo "Step 1: Enable HTTPS on your tailnet (one-time setup)"
  echo "  Go to: https://login.tailscale.com/admin/dns"
  echo "  Scroll to 'HTTPS Certificates' and click 'Enable HTTPS'"
  echo ""
  echo "Step 2: Create tags (one-time setup)"
  echo "  Go to: https://login.tailscale.com/admin/acls/tags"
  echo ""
  echo "  First tag (for the operator):"
  echo "    1. Click 'Create tag'"
  echo "    2. Tag name: k8s-operator"
  echo "    3. Owner: Select yourself (or your admin group)"
  echo "    4. Click 'Create'"
  echo ""
  echo "  Second tag (for devices the operator creates):"
  echo "    1. Click 'Create tag'"
  echo "    2. Tag name: k8s"
  echo "    3. Owner: tag:k8s-operator"
  echo "    4. Click 'Create'"
  echo ""
  echo "Step 3: Create OAuth client"
  echo "  1. Go to: https://login.tailscale.com/admin/settings/oauth"
  echo "  2. Click 'Generate OAuth client'"
  echo "  3. Description: InferaDB Dev Cluster (or similar)"
  echo "  4. Add scopes:"
  echo "     - Devices → Core: Read & Write, tag: k8s-operator"
  echo "     - Keys → Auth Keys: Read & Write, tag: k8s-operator"
  echo "  5. Click 'Generate client'"
  echo "  6. Copy the Client ID and Client Secret"
  echo ""

  # Check if running interactively
  if [ -t 0 ]; then
    echo "Enter your Tailscale OAuth credentials (will be cached for future use):"
    echo ""
    read -rp "Client ID: " TAILSCALE_CLIENT_ID
    read -rsp "Client Secret: " TAILSCALE_CLIENT_SECRET
    echo ""

    if [ -z "${TAILSCALE_CLIENT_ID}" ] || [ -z "${TAILSCALE_CLIENT_SECRET}" ]; then
      echo "ERROR: Both Client ID and Client Secret are required."
      exit 1
    fi

    # Save the credentials
    save_credentials
    echo ""
  else
    # Non-interactive mode - show instructions and exit
    echo "Set credentials via environment variables:"
    echo "  export TAILSCALE_CLIENT_ID=<client-id>"
    echo "  export TAILSCALE_CLIENT_SECRET=<client-secret>"
    echo "  ./scripts/dev-up.sh"
    echo ""
    echo "Or run interactively to enter credentials."
    exit 1
  fi
fi

# Save credentials for future use if they came from environment
if [ ! -f "${TAILSCALE_CREDS_FILE}" ]; then
  save_credentials
fi

# Check for Helm
if ! command -v helm &> /dev/null; then
  echo "ERROR: Helm is not installed."
  echo ""
  echo "Install Helm:"
  echo "  macOS:  brew install helm"
  echo "  Linux:  https://helm.sh/docs/intro/install/"
  exit 1
fi

echo "Prerequisites OK"
echo ""

echo "=== Creating local Talos cluster for InferaDB development ==="
echo "Cluster name: ${CLUSTER_NAME}"
echo "Build images: ${BUILD_IMAGES}"
echo ""

# Check if cluster already exists (by checking for Docker containers)
if docker ps -a --filter "name=${CLUSTER_NAME}" --format '{{.Names}}' 2>/dev/null | grep -q "${CLUSTER_NAME}"; then
  echo "Cluster '${CLUSTER_NAME}' already exists."
  echo "To recreate it, first run: ./scripts/dev-down.sh"
  exit 1
fi

# Clean up any stale talosctl contexts to prevent "-1" suffix on new cluster
for ctx in $(talosctl config contexts 2>/dev/null | awk '{print $2}' | grep "^${CLUSTER_NAME}"); do
  echo "Cleaning up stale talosctl context: ${ctx}..."
  talosctl config context "" 2>/dev/null || true
  talosctl config remove "${ctx}" --noconfirm 2>/dev/null || true
done

# Clean up stale kubectl contexts
if kubectl config get-contexts -o name 2>/dev/null | grep -q "^admin@${CLUSTER_NAME}"; then
  echo "Cleaning up stale kubectl context..."
  kubectl config delete-context "${KUBE_CONTEXT}" 2>/dev/null || true
  kubectl config delete-cluster "${CLUSTER_NAME}" 2>/dev/null || true
  kubectl config delete-user "${KUBE_CONTEXT}" 2>/dev/null || true
fi

# Create cluster using talosctl (Docker provisioner)
# Uses default flannel CNI for simplicity in local dev
# Production uses Cilium via Flux
echo "Creating Talos cluster with Docker provisioner..."

talosctl cluster create \
  --name "${CLUSTER_NAME}" \
  --workers 1 \
  --controlplanes 1 \
  --provisioner docker \
  --kubernetes-version 1.32.0 \
  --wait-timeout 10m

echo ""
echo "Cluster created successfully!"
echo ""

# Set kubectl context
echo "Setting kubectl context to ${KUBE_CONTEXT}..."
kubectl config use-context "${KUBE_CONTEXT}"

# Verify cluster is ready
echo "Verifying cluster is ready..."
kubectl get nodes

echo ""

# Bootstrap Flux (simplified for dev - no GitHub, use local path)
FLUX_DIR="${DEPLOY_DIR}/flux/clusters/dev-local/flux-system"

echo "Bootstrapping Flux..."
if [ -f "${FLUX_DIR}/gotk-components.yaml" ]; then
  kubectl apply -f "${FLUX_DIR}/gotk-components.yaml"
  kubectl apply -f "${FLUX_DIR}/gotk-sync.yaml"
  echo "Flux bootstrapped successfully!"
else
  echo "Note: Flux manifests not found at ${FLUX_DIR}/"
  echo "Skipping Flux bootstrap. Generate with: flux install --export > gotk-components.yaml"
fi

echo ""

# Deploy InferaDB applications
echo "=== Deploying InferaDB applications ==="
echo ""

# Get the repo root (parent of deploy/)
REPO_ROOT="${DEPLOY_DIR}/.."

# Set up local registry for image loading into Talos cluster
# The registry runs on the same Docker network as the Talos containers
REGISTRY_NAME="inferadb-registry"
REGISTRY_PORT=5050

# Start or reuse local registry
if docker ps --filter "name=${REGISTRY_NAME}" --format '{{.Names}}' | grep -q "${REGISTRY_NAME}"; then
  echo "Using existing local registry..."
else
  echo "Starting local registry for image loading..."
  # Get the Docker network used by Talos cluster
  TALOS_NETWORK=$(docker network ls --filter "name=${CLUSTER_NAME}" --format '{{.Name}}' | head -1)
  if [ -z "${TALOS_NETWORK}" ]; then
    TALOS_NETWORK="${CLUSTER_NAME}"
  fi

  docker run -d \
    --name "${REGISTRY_NAME}" \
    --network "${TALOS_NETWORK}" \
    -p ${REGISTRY_PORT}:5000 \
    --restart always \
    registry:2

  # Wait for registry to be ready
  echo "Waiting for registry to be ready..."
  sleep 3
fi

# Get registry IP on the Talos network for in-cluster access
REGISTRY_IP=$(docker inspect "${REGISTRY_NAME}" --format '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' | head -1)
echo "Registry available at ${REGISTRY_IP}:5000 (in-cluster) and localhost:${REGISTRY_PORT} (host)"

# Configure Talos nodes to allow insecure registry (HTTP instead of HTTPS)
echo "Configuring Talos nodes for insecure registry access..."
TALOS_CONTROLPLANE_IP=$(docker inspect "${CLUSTER_NAME}-controlplane-1" --format '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' | head -1)
TALOS_WORKER_IP=$(docker inspect "${CLUSTER_NAME}-worker-1" --format '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' | head -1)

cat > /tmp/talos-registry-patch.yaml <<EOF
machine:
  registries:
    mirrors:
      ${REGISTRY_IP}:5000:
        endpoints:
          - http://${REGISTRY_IP}:5000
    config:
      ${REGISTRY_IP}:5000:
        tls:
          insecureSkipVerify: true
EOF

# Apply registry config to both Talos nodes
for NODE_IP in "${TALOS_CONTROLPLANE_IP}" "${TALOS_WORKER_IP}"; do
  if [ -n "${NODE_IP}" ]; then
    echo "  Patching Talos node ${NODE_IP}..."
    talosctl patch machineconfig --nodes "${NODE_IP}" --patch @/tmp/talos-registry-patch.yaml 2>/dev/null || true
  fi
done
rm -f /tmp/talos-registry-patch.yaml
echo ""

# Build and push container images unless skipped
if [ "${BUILD_IMAGES}" = true ]; then
  echo "Building and pushing container images..."
  echo ""

  # Build engine image
  echo "Building inferadb-engine image..."
  if [ -f "${REPO_ROOT}/engine/Dockerfile" ]; then
    docker build -t inferadb-engine:latest "${REPO_ROOT}/engine"
    docker tag inferadb-engine:latest "localhost:${REGISTRY_PORT}/inferadb-engine:latest"
    docker push "localhost:${REGISTRY_PORT}/inferadb-engine:latest"
    echo "Engine image built and pushed!"
  else
    echo "Warning: engine/Dockerfile not found, skipping..."
  fi

  # Build control image
  echo "Building inferadb-control image..."
  if [ -f "${REPO_ROOT}/control/Dockerfile" ]; then
    docker build -t inferadb-control:latest "${REPO_ROOT}/control"
    docker tag inferadb-control:latest "localhost:${REGISTRY_PORT}/inferadb-control:latest"
    docker push "localhost:${REGISTRY_PORT}/inferadb-control:latest"
    echo "Control image built and pushed!"
  else
    echo "Warning: control/Dockerfile not found, skipping..."
  fi

  # Build dashboard image
  echo "Building inferadb-dashboard image..."
  if [ -f "${REPO_ROOT}/dashboard/Dockerfile" ]; then
    docker build -t inferadb-dashboard:latest "${REPO_ROOT}/dashboard"
    docker tag inferadb-dashboard:latest "localhost:${REGISTRY_PORT}/inferadb-dashboard:latest"
    docker push "localhost:${REGISTRY_PORT}/inferadb-dashboard:latest"
    echo "Dashboard image built and pushed!"
  else
    echo "Warning: dashboard/Dockerfile not found, skipping..."
  fi

  echo ""
else
  echo "Skipping image builds (--skip-build specified)"
  echo "Note: Images must already exist in registry at localhost:${REGISTRY_PORT}"
  echo ""
fi

# Create namespaces first (before installing components that use them)
echo "Creating namespaces..."
kubectl create namespace inferadb --dry-run=client -o yaml | kubectl apply -f -
kubectl create namespace fdb-system --dry-run=client -o yaml | kubectl apply -f -
kubectl create namespace local-path-storage --dry-run=client -o yaml | kubectl apply -f -
kubectl create namespace tailscale-system --dry-run=client -o yaml | kubectl apply -f -

# Label namespaces for privileged workloads BEFORE deploying pods
# This prevents PodSecurity warnings during deployment
kubectl label namespace fdb-system pod-security.kubernetes.io/enforce=privileged --overwrite
kubectl label namespace fdb-system pod-security.kubernetes.io/warn=privileged --overwrite
kubectl label namespace inferadb pod-security.kubernetes.io/enforce=privileged --overwrite
kubectl label namespace inferadb pod-security.kubernetes.io/warn=privileged --overwrite
kubectl label namespace local-path-storage pod-security.kubernetes.io/enforce=privileged --overwrite
kubectl label namespace local-path-storage pod-security.kubernetes.io/warn=privileged --overwrite
kubectl label namespace tailscale-system pod-security.kubernetes.io/enforce=privileged --overwrite
kubectl label namespace tailscale-system pod-security.kubernetes.io/warn=privileged --overwrite

# Install local-path-provisioner for PVC storage (Talos doesn't include a default StorageClass)
# Namespace already created and labeled above to avoid PodSecurity warnings
echo "Installing local-path-provisioner..."
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.26/deploy/local-path-storage.yaml
kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

# =============================================================================
# Tailscale Kubernetes Operator
# =============================================================================
echo "=== Installing Tailscale Kubernetes Operator ==="

# Install Tailscale Operator via Helm
echo "Adding Tailscale Helm repository..."
helm repo add tailscale https://pkgs.tailscale.com/helmcharts 2>/dev/null || true
helm repo update tailscale

echo "Installing Tailscale Operator..."
helm upgrade --install tailscale-operator tailscale/tailscale-operator \
  --namespace tailscale-system \
  --set oauth.clientId="${TAILSCALE_CLIENT_ID}" \
  --set oauth.clientSecret="${TAILSCALE_CLIENT_SECRET}" \
  --set apiServerProxyConfig.mode=noauth \
  --wait --timeout 5m

echo "Tailscale Operator installed!"
echo ""

# Install FDB operator from GitHub (no Helm chart published)
echo "Installing FoundationDB operator..."
FDB_OPERATOR_VERSION="v2.19.0"
FDB_RAW_URL="https://raw.githubusercontent.com/FoundationDB/fdb-kubernetes-operator/${FDB_OPERATOR_VERSION}"

# Install CRDs first
kubectl apply -f "${FDB_RAW_URL}/config/crd/bases/apps.foundationdb.org_foundationdbclusters.yaml"
kubectl apply -f "${FDB_RAW_URL}/config/crd/bases/apps.foundationdb.org_foundationdbbackups.yaml"
kubectl apply -f "${FDB_RAW_URL}/config/crd/bases/apps.foundationdb.org_foundationdbrestores.yaml"

# Wait for CRDs to be established
echo "Waiting for FoundationDB CRDs..."
kubectl wait --for=condition=established --timeout=60s crd/foundationdbclusters.apps.foundationdb.org

# Install RBAC (ClusterRole + Role from config/rbac/)
kubectl apply -f "${FDB_RAW_URL}/config/rbac/cluster_role.yaml"
kubectl apply -f "${FDB_RAW_URL}/config/rbac/role.yaml" -n fdb-system

# Install the operator deployment
# The upstream manager.yaml references serviceAccountName: fdb-kubernetes-operator-controller-manager
# but the ServiceAccount is named controller-manager (kustomize would add the prefix)
# Also configure it to watch all namespaces (default is single namespace mode)
curl -s "${FDB_RAW_URL}/config/deployment/manager.yaml" | \
  sed 's/serviceAccountName: fdb-kubernetes-operator-controller-manager/serviceAccountName: controller-manager/' | \
  sed '/WATCH_NAMESPACE/,/fieldPath:/d' | \
  kubectl apply -n fdb-system -f -

# Create role bindings with correct ClusterRole references
# Note: The ClusterRoles from config/rbac/ are named manager-role and manager-clusterrole
# (without the fdb-kubernetes-operator- prefix that kustomize would add)
# For global mode (no WATCH_NAMESPACE), we need ClusterRoleBindings for cluster-wide access
kubectl create clusterrolebinding fdb-operator-manager-role-global \
  --clusterrole=manager-role \
  --serviceaccount=fdb-system:controller-manager \
  --dry-run=client -o yaml | kubectl apply -f -

kubectl create clusterrolebinding fdb-operator-manager-clusterrolebinding \
  --clusterrole=manager-clusterrole \
  --serviceaccount=fdb-system:controller-manager \
  --dry-run=client -o yaml | kubectl apply -f -

# Create RBAC for FDB sidecar pods (they need to read/patch pods for annotations)
echo "Creating FDB sidecar RBAC..."
cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: fdb-sidecar
  namespace: inferadb
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch", "patch", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: fdb-sidecar
  namespace: inferadb
subjects:
- kind: ServiceAccount
  name: default
  namespace: inferadb
roleRef:
  kind: Role
  name: fdb-sidecar
  apiGroup: rbac.authorization.k8s.io
EOF

# Wait for operator to be ready with progress feedback
echo "Waiting for FDB operator to be ready..."
echo "(Init containers download FDB binaries, this may take a few minutes)"
WAIT_TIMEOUT=300
WAIT_INTERVAL=10
ELAPSED=0
while [ $ELAPSED -lt $WAIT_TIMEOUT ]; do
  # Show current pod status
  POD_STATUS=$(kubectl get pods -n fdb-system -o wide --no-headers 2>/dev/null || echo "Unable to get pod status")
  echo "  [$ELAPSED/${WAIT_TIMEOUT}s] $POD_STATUS"

  # Check if deployment is available
  if kubectl wait --for=condition=available --timeout=1s deployment/controller-manager -n fdb-system 2>/dev/null; then
    echo "FDB operator is ready!"
    break
  fi

  sleep $WAIT_INTERVAL
  ELAPSED=$((ELAPSED + WAIT_INTERVAL))
done

if [ $ELAPSED -ge $WAIT_TIMEOUT ]; then
  echo "ERROR: FDB operator did not become ready within ${WAIT_TIMEOUT}s"
  echo "Checking pod events for diagnostics..."
  kubectl describe pod -n fdb-system -l app=fdb-kubernetes-operator 2>/dev/null | grep -A 20 "Events:" || true
  kubectl logs -n fdb-system -l app=fdb-kubernetes-operator --tail=50 2>/dev/null || true
  exit 1
fi

# Apply apps (dev overlay) with registry IP substitution
echo "Deploying InferaDB applications..."

# Create a kustomization patch for the dev registry
# The registry is accessible from inside the cluster at ${REGISTRY_IP}:5000
cat > "${DEPLOY_DIR}/flux/apps/dev/registry-patch.yaml" <<EOF
# Auto-generated by dev-up.sh - patches images to use local registry
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inferadb-engine
  namespace: inferadb
spec:
  template:
    spec:
      containers:
        - name: inferadb-engine
          image: ${REGISTRY_IP}:5000/inferadb-engine:latest
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inferadb-control
  namespace: inferadb
spec:
  template:
    spec:
      containers:
        - name: inferadb-control
          image: ${REGISTRY_IP}:5000/inferadb-control:latest
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inferadb-dashboard
  namespace: inferadb
spec:
  template:
    spec:
      containers:
        - name: inferadb-dashboard
          image: ${REGISTRY_IP}:5000/inferadb-dashboard:latest
EOF

# Note: registry-patch.yaml is referenced in kustomization.yaml patches section
# The file is generated above with the current registry IP

# The dev overlay includes both base resources AND Tailscale ingress
kubectl apply -k "${DEPLOY_DIR}/flux/apps/dev"

echo ""
echo "Applications deployed!"
echo ""
echo "Note: It may take a few minutes for all pods to be ready."
echo "Monitor progress with: kubectl get pods -n inferadb -w"

# Get the tailnet name for display
echo ""
echo "Waiting for Tailscale ingress to be ready..."
sleep 10

# Try to get the actual tailnet domain from the ingress status
INGRESS_HOSTNAME=$(kubectl get ingress inferadb-api-tailscale -n inferadb -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")

# Determine the tailnet suffix for display
if [ -n "${INGRESS_HOSTNAME}" ]; then
  # Extract from ingress status (most accurate)
  TAILNET_SUFFIX=$(echo "${INGRESS_HOSTNAME}" | sed 's/^inferadb-api\.//')
elif DETECTED_TAILNET=$(get_tailnet_info 2>/dev/null); then
  # Fall back to local CLI detection
  TAILNET_SUFFIX="${DETECTED_TAILNET}"
else
  TAILNET_SUFFIX=""
fi

echo ""
echo "=== Development environment ready! ==="
echo ""
echo "Cluster context: ${KUBE_CONTEXT}"
echo ""
if [ -n "${TAILNET_SUFFIX}" ]; then
  echo "Access your services:"
  echo "  https://inferadb-api.${TAILNET_SUFFIX}       - API"
  echo "  https://inferadb-dashboard.${TAILNET_SUFFIX} - Dashboard"
else
  echo "Access your services:"
  echo "  https://inferadb-api.<your-tailnet>.ts.net       - API"
  echo "  https://inferadb-dashboard.<your-tailnet>.ts.net - Dashboard"
  echo ""
  echo "  (Check 'kubectl get ingress -n inferadb' for exact URLs once ready)"
fi
echo ""
echo "API routes:"
echo "  /control/v1/*       -> Control (auth, organizations, users, vaults, tokens)"
echo "  /access/v1/*        -> Engine (evaluate, relationships, expand, lookup)"
echo "  /.well-known/*      -> Control (JWKS discovery)"
echo "  /healthz, /readyz   -> Control (health checks)"
echo ""
echo "Useful commands:"
echo "  kubectl get pods -A                    # List all pods"
echo "  kubectl get pods -n inferadb           # List InferaDB pods"
echo "  kubectl get ingress -n inferadb        # Show ingress routes (with Tailscale URLs)"
echo "  talosctl dashboard --nodes 10.5.0.2    # Talos dashboard"
echo ""
echo "To destroy this cluster, run:"
echo "  ./scripts/dev-down.sh"
echo ""
